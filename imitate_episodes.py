import torch
import numpy as np
import os
import sys
sys.path.insert(0, os.path.dirname(__file__))
import pickle
import argparse
import matplotlib.pyplot as plt
from copy import deepcopy
from tqdm import tqdm
from einops import rearrange
import yaml

# from constants import DT
# from constants import PUPPET_GRIPPER_JOINT_OPEN
# from utils import load_data # data functions  # DEPRECATED: Use HDF5Loader instead
# from utils import sample_box_pose, sample_insertion_pose # robot functions
from utils import compute_dict_mean, set_seed, detach_dict # helper functions
from policy import ACTPolicy, CNNMLPPolicy
# from visualize_episodes import save_videos

# Import new HDF5Loader
from dataset.hdf5_loader import HDF5Loader
from dataset.reader import ActionType, ObservationType

# from sim_env import BOX_POSE
import re

def main(args):

    # Device configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  Using device: {device}")
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print()

    # æ‰“å°å…³é”®å‚æ•°ç¡®è®¤
    print("ğŸš€ å¼€å§‹è®­ç»ƒè¿›ç¨‹...")
    print(f"ğŸ“‹ ä»»åŠ¡: {args['task_name']}")
    print(f"âš™ï¸  æ¨¡å¼: {'è¯„ä¼°' if args['eval'] else 'è®­ç»ƒ'}")
    print(f"ğŸ¯ ç­–ç•¥: {args['policy_class']}")
    print(f"ğŸ“ æ‰¹å¤§å°: {args['batch_size']}")
    print(f"ğŸ”¢ è½®æ•°: {args['num_epochs']}")
    print(f"ğŸ“ˆ å­¦ä¹ ç‡: {args['lr']:.6f}")
    print()

    ckpt_dir = args['ckpt_dir']
    policy_class = args['policy_class']
    onscreen_render = args['onscreen_render']
    task_name = args['task_name']
    batch_size_train = args['batch_size']
    batch_size_val = args['batch_size']
    num_epochs = args['num_epochs']

    # get task parameters from YAML only (no SIM_TASK_CONFIGS fallback)
    is_sim = task_name[:4] == 'sim_' or task_name.startswith('fr3_') or task_name.startswith('monte01_')
    task_config = args.get('_task_config')
    if task_config is None:
        raise KeyError(
            f"Task config not provided for task '{task_name}'. "
            f"Ensure you load YAML via task_config_manager and pass it to main (args['_task_config'])."
        )
    dataset_dir = task_config['dataset_dir']
    num_episodes = task_config.get('num_episodes', None)  # Optional, auto-detect if not specified
    episode_len = task_config['episode_len']
    camera_names = task_config['camera_names']

    # get state dimension from task config or default to 14
    state_dim = task_config.get('state_dim', 14)

    # Derive obs->action pair from ckpt_dir suffix, e.g., *_ee2ee, *_q2q
    # Supported keys: 'ee' (end-effector pose), 'q' (joint position)
    pair_match = re.search(r'(ee|q)2(ee|q)(?![a-zA-Z0-9])', ckpt_dir)
    if pair_match:
        obs_key, act_key = pair_match.group(1), pair_match.group(2)
    else:
        # Default to joint->joint if no suffix specified
        obs_key, act_key = 'q', 'q'

    # Map to enums
    obs_type_from_suffix = ObservationType.END_EFFECTOR_POSE if obs_key == 'ee' else ObservationType.JOINT_POSITION_ONLY
    action_type_from_suffix = ActionType.END_EFFECTOR_POSE if act_key == 'ee' else ActionType.JOINT_POSITION

    # Control mode controls how dataset constructs (obs, action)
    if obs_key == 'ee' and act_key == 'ee':
        # EE pose: [x, y, z, qx, qy, qz, qw, gripper] = 8 dimensions
        state_dim = 8
        print(f"Using ee2ee mode inferred from ckpt_dir, state_dim set to {state_dim}")
    elif obs_key == 'q' and act_key == 'q':
        # Joint position mode: use state_dim from task config (default: 8 for FR3)
        # state_dim already set from task_config above (line 70)
        print(f"Using q2q (joint position) mode inferred from ckpt_dir, state_dim = {state_dim}")
    else:
        # Mixed modes (ee2q or q2ee) require dataset/model changes; guard for now
        raise NotImplementedError(
            f"Requested mixed mode '{obs_key}2{act_key}' inferred from ckpt_dir is not supported yet."
        )

    lr_backbone = 1e-5
    backbone = 'resnet18'
    if policy_class == 'ACT':
        enc_layers = 4
        dec_layers = 7
        nheads = 8
        policy_config = {'lr': args['lr'],
                         'num_queries': args['chunk_size'],
                         'kl_weight': args['kl_weight'],
                         'hidden_dim': args['hidden_dim'],
                         'dim_feedforward': args['dim_feedforward'],
                         'lr_backbone': lr_backbone,
                         'backbone': backbone,
                         'enc_layers': enc_layers,
                         'dec_layers': dec_layers,
                         'nheads': nheads,
                         'camera_names': camera_names,
                         'state_dim': state_dim,
                         }
    elif policy_class == 'CNNMLP':
        policy_config = {'lr': args['lr'], 'lr_backbone': lr_backbone, 'backbone' : backbone, 'num_queries': 1,
                         'camera_names': camera_names, 'state_dim': state_dim,}
    else:
        raise NotImplementedError

    config = {
        'num_epochs': num_epochs,
        'ckpt_dir': ckpt_dir,
        'episode_len': episode_len,
        'state_dim': state_dim,
        'lr': args['lr'],
        'policy_class': policy_class,
        'onscreen_render': onscreen_render,
        'policy_config': policy_config,
        'task_name': task_name,
        'seed': args['seed'],
        'temporal_agg': args['temporal_agg'],
        'camera_names': camera_names,
        'real_robot': not is_sim,
        'device': device,
        'stats': None  # Will be filled after data loading
    }

    # åŠ è½½å…‰ç…§å¢å¼ºé…ç½®
    augmentation_config = None
    lighting_config_path = os.path.join(os.path.dirname(__file__), 'configs', 'lighting_augmentation.yaml')
    if os.path.exists(lighting_config_path):
        with open(lighting_config_path, 'r', encoding='utf-8') as f:
            augmentation_config = yaml.safe_load(f)
        print(f"âœ… å·²åŠ è½½å…‰ç…§å¢å¼ºé…ç½®: {lighting_config_path}")
    else:
        print(f"âš ï¸  å…‰ç…§å¢å¼ºé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {lighting_config_path}")

    # Create HDF5 data loader with control mode support
    # Determine action type and observation type from control mode
    action_type = action_type_from_suffix
    observation_type = obs_type_from_suffix

    hdf5_loader_config = {
        'num_episodes': num_episodes,
        'camera_names': camera_names,
        'batch_size_train': batch_size_train,
        'batch_size_val': batch_size_val,
        'episode_len': episode_len,
        'augmentation_config': augmentation_config,
    }

    hdf5_loader = HDF5Loader(
        config=hdf5_loader_config,
        dataset_dir=dataset_dir,
        action_type=action_type,
        observation_type=observation_type
    )

    train_dataloader, val_dataloader, stats, _ = hdf5_loader.create_dataloaders()

    # Add stats to config for eval
    config['stats'] = stats

    # save dataset stats
    if not os.path.isdir(ckpt_dir):
        os.makedirs(ckpt_dir)
    stats_path = os.path.join(ckpt_dir, f'dataset_stats.pkl')
    with open(stats_path, 'wb') as f:
        pickle.dump(stats, f)

    best_ckpt_info = train_bc(train_dataloader, val_dataloader, config)
    best_epoch, min_val_loss, best_state_dict = best_ckpt_info

    # save best checkpoint
    ckpt_path = os.path.join(ckpt_dir, f'policy_best.ckpt')
    torch.save(best_state_dict, ckpt_path)
    print(f'Best ckpt, val loss {min_val_loss:.6f} @ epoch{best_epoch}')


def make_policy(policy_class, policy_config):
    if policy_class == 'ACT':
        policy = ACTPolicy(policy_config)
    elif policy_class == 'CNNMLP':
        policy = CNNMLPPolicy(policy_config)
    else:
        raise NotImplementedError
    return policy


def make_optimizer(policy_class, policy):
    if policy_class == 'ACT':
        optimizer = policy.configure_optimizers()
    elif policy_class == 'CNNMLP':
        optimizer = policy.configure_optimizers()
    else:
        raise NotImplementedError
    return optimizer


def get_image(ts, camera_names, device):
    """
    Get images from observation and return as dict for policy inference

    Returns:
        dict: {cam_name: torch.Tensor (C, H, W)} on specified device
    """
    image_dict = {}
    for cam_name in camera_names:
        curr_image = rearrange(ts.observation['images'][cam_name], 'h w c -> c h w')
        # Normalize to [0, 1] and convert to torch tensor
        curr_image = torch.from_numpy(curr_image / 255.0).float().to(device)
        image_dict[cam_name] = curr_image
    return image_dict

def forward_pass(data, policy, device):
    image_data, qpos_data, action_data, is_pad = data
    image_data = image_data.to(device)
    qpos_data = qpos_data.to(device)
    action_data = action_data.to(device)
    is_pad = is_pad.to(device)
    return policy(qpos_data, image_data, action_data, is_pad)


def train_bc(train_dataloader, val_dataloader, config):
    num_epochs = config['num_epochs']
    ckpt_dir = config['ckpt_dir']
    seed = config['seed']
    policy_class = config['policy_class']
    policy_config = config['policy_config']
    device = config['device']

    set_seed(seed)

    print("ğŸ—ï¸  åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨...")
    policy = make_policy(policy_class, policy_config)
    policy.to(device)
    optimizer = make_optimizer(policy_class, policy)

    # æ‰“å°æ¨¡å‹å’Œä¼˜åŒ–å™¨ä¿¡æ¯
    total_params = sum(p.numel() for p in policy.parameters())
    trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)

    print(f"ğŸ§  æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°æ•°é‡:   {total_params/1e6:.2f}M")
    print(f"  å¯è®­ç»ƒå‚æ•°:   {trainable_params/1e6:.2f}M")
    print(f"  å‚æ•°åˆ©ç”¨ç‡:   {trainable_params/total_params*100:.1f}%")

    print(f"âš¡ ä¼˜åŒ–å™¨é…ç½®:")
    print(f"  ä¼˜åŒ–å™¨ç±»å‹:   {type(optimizer).__name__}")
    for group in optimizer.param_groups:
        print(f"  å­¦ä¹ ç‡:       {group['lr']:.8f}")
        if 'weight_decay' in group:
            print(f"  æƒé‡è¡°å‡:     {group['weight_decay']}")
        break  # åªæ‰“å°ç¬¬ä¸€ä¸ªå‚æ•°ç»„
    print()

    train_history = []
    validation_history = []
    min_val_loss = np.inf
    best_ckpt_info = None

    print("=" * 60)
    print("ğŸ“ å¼€å§‹è®­ç»ƒå¾ªç¯")
    print("=" * 60)

    for epoch in tqdm(range(num_epochs)):
        print(f'\nEpoch {epoch}')
        # validation
        with torch.inference_mode():
            policy.eval()
            epoch_dicts = []
            for batch_idx, data in enumerate(val_dataloader):
                forward_dict = forward_pass(data, policy, device)
                epoch_dicts.append(forward_dict)
            epoch_summary = compute_dict_mean(epoch_dicts)
            validation_history.append(epoch_summary)

            epoch_val_loss = epoch_summary['loss']
            if epoch_val_loss < min_val_loss:
                min_val_loss = epoch_val_loss
                best_ckpt_info = (epoch, min_val_loss, deepcopy(policy.state_dict()))
        print(f'Val loss:   {epoch_val_loss:.5f}')
        summary_string = ''
        for k, v in epoch_summary.items():
            summary_string += f'{k}: {v.item():.3f} '
        print(summary_string)

        # training
        policy.train()
        optimizer.zero_grad()
        train_epoch_dicts = []  # Per-epoch accumulator
        for batch_idx, data in enumerate(train_dataloader):
            forward_dict = forward_pass(data, policy, device)
            # backward
            loss = forward_dict['loss']
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            train_history.append(detach_dict(forward_dict))
            train_epoch_dicts.append(detach_dict(forward_dict))
        # Compute epoch summary from this epoch's batches
        epoch_summary = compute_dict_mean(train_epoch_dicts)
        epoch_train_loss = epoch_summary['loss']
        print(f'Train loss: {epoch_train_loss:.5f}')
        summary_string = ''
        for k, v in epoch_summary.items():
            summary_string += f'{k}: {v.item():.3f} '
        print(summary_string)

        if epoch % 100 == 0:
            ckpt_path = os.path.join(ckpt_dir, f'policy_epoch_{epoch}_seed_{seed}.ckpt')
            torch.save(policy.state_dict(), ckpt_path)
            plot_history(train_history, validation_history, epoch, ckpt_dir, seed)

    ckpt_path = os.path.join(ckpt_dir, f'policy_last.ckpt')
    torch.save(policy.state_dict(), ckpt_path)

    best_epoch, min_val_loss, best_state_dict = best_ckpt_info
    ckpt_path = os.path.join(ckpt_dir, f'policy_epoch_{best_epoch}_seed_{seed}.ckpt')
    torch.save(best_state_dict, ckpt_path)
    print(f'Training finished:\nSeed {seed}, val loss {min_val_loss:.6f} at epoch {best_epoch}')

    # save training curves
    plot_history(train_history, validation_history, num_epochs, ckpt_dir, seed)

    return best_ckpt_info


def plot_history(train_history, validation_history, num_epochs, ckpt_dir, seed):
    # save training curves
    for key in train_history[0]:
        plot_path = os.path.join(ckpt_dir, f'train_val_{key}_seed_{seed}.png')
        plt.figure()
        train_values = [summary[key].item() for summary in train_history]
        val_values = [summary[key].item() for summary in validation_history]
        plt.plot(np.linspace(0, num_epochs-1, len(train_history)), train_values, label='train')
        plt.plot(np.linspace(0, num_epochs-1, len(validation_history)), val_values, label='validation')
        # plt.ylim([-0.1, 1])
        plt.tight_layout()
        plt.legend()
        plt.title(key)
        plt.savefig(plot_path)
        plt.close()  # é‡Šæ”¾å†…å­˜ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
    print(f'Saved plots to {ckpt_dir}')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ACTè®­ç»ƒè„šæœ¬ - ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†å‚æ•°')
    parser.add_argument('--config', type=str, required=True,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: configs/tasks/fr3_bs_0916_50ep_ds.yaml)')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    try:
        from task_config_manager import load_task_config
        config_data = load_task_config(args.config, eval_mode=False)

        print(f"âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {args.config}")
        print(f"  ä»»åŠ¡å: {config_data['args']['task_name']}")
        print(f"  æœºå™¨äºº: {config_data['config']['robot']['name']}")
        print(f"  æ•°æ®é›†: {config_data['task_config']['dataset_dir']}")
        print(f"  æ¨¡å¼: è®­ç»ƒ")
        print()

        # è¯¦ç»†æ‰“å°è®­ç»ƒè¶…å‚æ•°
        print("=" * 60)
        print("ğŸ”§ è®­ç»ƒè¶…å‚æ•°é…ç½®")
        print("=" * 60)

        args_data = config_data['args']
        task_data = config_data['task_config']

        print("ğŸ“Š åŸºç¡€è®­ç»ƒå‚æ•°:")
        print(f"  ç­–ç•¥ç±»å‹:     {args_data['policy_class']}")
        print(f"  æ‰¹å¤§å°:       {args_data['batch_size']}")
        print(f"  è®­ç»ƒè½®æ•°:     {args_data['num_epochs']}")
        print(f"  å­¦ä¹ ç‡:       {args_data['lr']:.6f}")
        print(f"  éšæœºç§å­:     {args_data['seed']}")
        print()

        print("ğŸ¤– ACTæ¨¡å‹å‚æ•°:")
        print(f"  KLæƒé‡:       {args_data['kl_weight']}")
        print(f"  åŠ¨ä½œå—å¤§å°:   {args_data['chunk_size']}")
        print(f"  éšè—å±‚ç»´åº¦:   {args_data['hidden_dim']}")
        print(f"  å‰é¦ˆç½‘ç»œç»´åº¦: {args_data['dim_feedforward']}")
        print()

        print("ğŸ“ æ•°æ®é…ç½®:")
        print(f"  çŠ¶æ€ç»´åº¦:     {task_data['state_dim']}")
        print(f"  Episodeé•¿åº¦:  {task_data['episode_len']}")
        print(f"  Episodeæ•°é‡:  {task_data.get('num_episodes', 'Auto-detect')}")
        print(f"  ç›¸æœº:         {task_data['camera_names']}")
        print()

        print("ğŸ’¾ å­˜å‚¨é…ç½®:")
        print(f"  æ£€æŸ¥ç‚¹ç›®å½•:   {args_data['ckpt_dir']}")
        print(f"  æ—¶åºèšåˆ:     {args_data['temporal_agg']}")
        print(f"  å±å¹•æ¸²æŸ“:     {args_data['onscreen_render']}")
        print()

        print("=" * 60)
        print()

        # è¿è¡Œä¸»å‡½æ•°ï¼ˆç»Ÿä¸€ä» YAML æ³¨å…¥ä»»åŠ¡é…ç½®ï¼‰
        args_data = config_data['args']
        args_data['_task_config'] = config_data['task_config']
        main(args_data)

    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®")
        sys.exit(1)
