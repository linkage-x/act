#!/usr/bin/env python3

import torch
import numpy as np
import h5py
import os
from abc import ABC, abstractmethod
from policy_inference import PolicyInference


class RobotInference(ABC):
    """æœºå™¨äººæ¨ç†æŠ½è±¡ç±»ï¼Œå®šä¹‰æœºå™¨äººç‰¹å®šçš„æ¥å£"""
    
    def __init__(self, policy_inference, task_config):
        """
        åˆå§‹åŒ–æœºå™¨äººæ¨ç†å™¨
        
        Args:
            policy_inference: ç­–ç•¥æ¨ç†å™¨å®ä¾‹ (ACTInference, PPOInferenceç­‰)
            task_config: ä»»åŠ¡é…ç½®å­—å…¸
        """
        self.policy_inference = policy_inference
        self.task_config = task_config
        
    @abstractmethod
    def preprocess_images(self, images_dict):
        """
        é¢„å¤„ç†å›¾åƒæ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼‰
        
        Args:
            images_dict: åŸå§‹å›¾åƒå­—å…¸
            
        Returns:
            processed_images: é¢„å¤„ç†åçš„å›¾åƒå­—å…¸
        """
        pass
        
    @abstractmethod
    def validate_input_data(self, state, images_dict):
        """
        éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼‰
        
        Args:
            state: æœºå™¨äººçŠ¶æ€
            images_dict: å›¾åƒå­—å…¸
            
        Raises:
            ValueError: å½“è¾“å…¥æ•°æ®æ ¼å¼ä¸æ­£ç¡®æ—¶
        """
        pass
        
    def predict(self, state, images_dict):
        """
        è¿›è¡ŒåŠ¨ä½œé¢„æµ‹
        
        Args:
            state: æœºå™¨äººçŠ¶æ€
            images_dict: ç›¸æœºå›¾åƒå­—å…¸
            
        Returns:
            predicted_actions: é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
        """
        # éªŒè¯è¾“å…¥æ•°æ®
        self.validate_input_data(state, images_dict)
        
        # é¢„å¤„ç†å›¾åƒ
        processed_images = self.preprocess_images(images_dict)
        
        # è°ƒç”¨ç­–ç•¥æ¨ç†
        return self.policy_inference.predict(state, processed_images)


class FR3Inference(RobotInference):
    """FR3æœºå™¨äººç‰¹å®šçš„æ¨ç†å™¨"""
    
    def __init__(self, policy_inference, task_name='fr3_peg_in_hole_extended'):
        """
        åˆå§‹åŒ–FR3æ¨ç†å™¨
        
        Args:
            policy_inference: ç­–ç•¥æ¨ç†å™¨å®ä¾‹
            task_name: ä»»åŠ¡åç§°
        """
        from constants import SIM_TASK_CONFIGS
        
        self.task_name = task_name
        
        if task_name not in SIM_TASK_CONFIGS:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡åç§°: {task_name}")
            
        task_config = SIM_TASK_CONFIGS[task_name]
        super().__init__(policy_inference, task_config)
        
    def preprocess_images(self, images_dict):
        """é¢„å¤„ç†FR3å›¾åƒæ•°æ®"""
        processed_images = {}
        
        for cam_name, image in images_dict.items():
            if cam_name not in self.task_config['camera_names']:
                continue
                
            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡® (H, W, C) -> (C, H, W)
            if len(image.shape) == 3 and image.shape[-1] == 3:
                image = image.transpose(2, 0, 1)
                
            # å½’ä¸€åŒ–åˆ° [0, 1]
            if image.max() > 1.0:
                image = image.astype(np.float32) / 255.0
                
            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            image_tensor = torch.from_numpy(image).float().to(self.policy_inference.device)
            if len(image_tensor.shape) == 3:
                image_tensor = image_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                
            processed_images[cam_name] = image_tensor
            
        return processed_images
        
    def validate_input_data(self, qpos, images_dict):
        """éªŒè¯FR3è¾“å…¥æ•°æ®æ ¼å¼"""
        # éªŒè¯qposç»´åº¦
        qpos = np.array(qpos)
        if qpos.shape[0] != self.task_config['state_dim']:
            raise ValueError(f"QPosç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.task_config['state_dim']}, å¾—åˆ°{qpos.shape[0]}")
            
        # éªŒè¯ç›¸æœºæ•°é‡
        if len(images_dict) != len(self.task_config['camera_names']):
            raise ValueError(f"ç›¸æœºæ•°é‡ä¸åŒ¹é…: æœŸæœ›{self.task_config['camera_names']}, å¾—åˆ°{list(images_dict.keys())}")
            
        # éªŒè¯ç›¸æœºåç§°
        for cam_name in images_dict.keys():
            if cam_name not in self.task_config['camera_names']:
                raise ValueError(f"æœªçŸ¥ç›¸æœºåç§°: {cam_name}")
                
    def predict_from_episode(self, episode_path, timestep=0):
        """
        ä»episodeæ–‡ä»¶ä¸­è¯»å–æ•°æ®å¹¶è¿›è¡Œé¢„æµ‹
        
        Args:
            episode_path: HDF5 episodeæ–‡ä»¶è·¯å¾„
            timestep: è¦é¢„æµ‹çš„æ—¶é—´æ­¥
            
        Returns:
            predicted_actions: é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
            ground_truth: çœŸå®åŠ¨ä½œ (ç”¨äºå¯¹æ¯”)
            qpos: å…³èŠ‚ä½ç½®
        """
        with h5py.File(episode_path, 'r') as f:
            # è¯»å–æ•°æ®
            qpos = f['observations/qpos'][timestep]
            
            # æ„å»ºå›¾åƒå­—å…¸ - ä½¿ç”¨é…ç½®ä¸­çš„ç›¸æœºåç§°
            images_dict = {}
            for cam_name in self.task_config['camera_names']:
                if f'observations/images/{cam_name}' in f:
                    images_dict[cam_name] = f[f'observations/images/{cam_name}'][timestep]
                else:
                    raise KeyError(f"ç›¸æœºæ•°æ®ä¸å­˜åœ¨: observations/images/{cam_name}")
            
            # çœŸå®åŠ¨ä½œ (ç”¨äºå¯¹æ¯”)
            if timestep < f['action'].shape[0]:
                ground_truth = f['action'][timestep]
            else:
                ground_truth = None
                
        # è¿›è¡Œé¢„æµ‹
        predicted_actions = self.predict(qpos, images_dict)
        
        return predicted_actions, ground_truth, qpos
        
    def evaluate_episode(self, episode_path, num_steps=10):
        """
        è¯„ä¼°æ•´ä¸ªepisodeçš„é¢„æµ‹æ€§èƒ½
        
        Args:
            episode_path: HDF5 episodeæ–‡ä»¶è·¯å¾„
            num_steps: è¦è¯„ä¼°çš„æ­¥æ•°
            
        Returns:
            avg_mse: å¹³å‡MSEè¯¯å·®
        """
        print(f"\nğŸ“Š è¯„ä¼°Episode: {episode_path}")
        
        mse_errors = []
        
        with h5py.File(episode_path, 'r') as f:
            episode_len = min(f['action'].shape[0], num_steps)
            
        for t in range(episode_len):
            pred_actions, gt_action, qpos = self.predict_from_episode(episode_path, t)
            
            if gt_action is not None:
                # è®¡ç®—MSEè¯¯å·® (åªæ¯”è¾ƒç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œå› ä¸ºchunk_size=100)
                mse = np.mean((pred_actions[0] - gt_action) ** 2)
                mse_errors.append(mse)
                
                if t % 5 == 0:  # æ¯5æ­¥æ‰“å°ä¸€æ¬¡
                    print(f"  æ­¥éª¤ {t:3d}: MSE={mse:.6f}")
                    print(f"    é¢„æµ‹: {pred_actions[0]}")
                    print(f"    çœŸå®: {gt_action}")
                    
        avg_mse = np.mean(mse_errors)
        print(f"\nğŸ“ˆ å¹³å‡MSEè¯¯å·®: {avg_mse:.6f}")
        return avg_mse


class RealtimeController:
    """é€šç”¨å®æ—¶æ§åˆ¶å™¨"""
    
    def __init__(self, robot_inference):
        """
        åˆå§‹åŒ–å®æ—¶æ§åˆ¶å™¨
        
        Args:
            robot_inference: æœºå™¨äººæ¨ç†å™¨å®ä¾‹
        """
        self.robot_inference = robot_inference
        self.action_buffer = None  # åŠ¨ä½œç¼“å†²åŒº
        self.buffer_index = 0      # å½“å‰åŠ¨ä½œç´¢å¼•
        
        print("ğŸ¤– å®æ—¶æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def get_robot_state(self):
        """
        è·å–æœºå™¨äººå½“å‰çŠ¶æ€ (éœ€è¦æ ¹æ®å®é™…æœºå™¨äººæ¥å£å®ç°)
        
        Returns:
            state: æœºå™¨äººçŠ¶æ€
        """
        raise NotImplementedError("éœ€è¦å­ç±»å®ç°get_robot_stateæ–¹æ³•")
        
    def get_camera_images(self):
        """
        è·å–ç›¸æœºå›¾åƒ (éœ€è¦æ ¹æ®å®é™…æœºå™¨äººæ¥å£å®ç°)
        
        Returns:
            images_dict: å›¾åƒå­—å…¸
        """
        raise NotImplementedError("éœ€è¦å­ç±»å®ç°get_camera_imagesæ–¹æ³•")
        
    def send_action_to_robot(self, action):
        """
        å‘é€åŠ¨ä½œåˆ°æœºå™¨äºº (éœ€è¦æ ¹æ®å®é™…æœºå™¨äººæ¥å£å®ç°)
        
        Args:
            action: åŠ¨ä½œå‘é‡
        """
        raise NotImplementedError("éœ€è¦å­ç±»å®ç°send_action_to_robotæ–¹æ³•")
        
    def step(self):
        """æ‰§è¡Œä¸€æ­¥æ§åˆ¶å¾ªç¯"""
        # å¦‚æœåŠ¨ä½œç¼“å†²åŒºä¸ºç©ºæˆ–å·²ç”¨å®Œï¼Œé‡æ–°é¢„æµ‹
        if self.action_buffer is None or self.buffer_index >= self.action_buffer.shape[0]:
            state = self.get_robot_state()
            images = self.get_camera_images()
            
            # é¢„æµ‹åŠ¨ä½œåºåˆ—
            self.action_buffer = self.robot_inference.predict(state, images)
            self.buffer_index = 0
            
            print(f"ğŸ”„ é‡æ–°é¢„æµ‹åŠ¨ä½œåºåˆ—ï¼Œé•¿åº¦: {self.action_buffer.shape[0]}")
            
        # æ‰§è¡Œå½“å‰åŠ¨ä½œ
        current_action = self.action_buffer[self.buffer_index]
        self.send_action_to_robot(current_action)
        
        self.buffer_index += 1
        
        return current_action
        
    def run_realtime_control(self, max_steps=1000, freq_hz=10):
        """
        è¿è¡Œå®æ—¶æ§åˆ¶å¾ªç¯
        
        Args:
            max_steps: æœ€å¤§æ­¥æ•°
            freq_hz: æ§åˆ¶é¢‘ç‡ (Hz)
        """
        import time
        
        dt = 1.0 / freq_hz
        
        print(f"ğŸš€ å¼€å§‹å®æ—¶æ§åˆ¶ï¼Œé¢‘ç‡: {freq_hz}Hz")
        
        for step in range(max_steps):
            start_time = time.time()
            
            # æ‰§è¡Œä¸€æ­¥æ§åˆ¶
            action = self.step()
            
            # æ§åˆ¶é¢‘ç‡
            elapsed = time.time() - start_time
            if elapsed < dt:
                time.sleep(dt - elapsed)
                
            if step % 50 == 0:
                print(f"æ­¥éª¤ {step}: åŠ¨ä½œ = {action}")
                
        print("âœ… å®æ—¶æ§åˆ¶å®Œæˆ")


class FR3RealtimeController(RealtimeController):
    """FR3ç‰¹å®šçš„å®æ—¶æ§åˆ¶å™¨"""
    
    def get_robot_state(self):
        """è·å–FR3æœºå™¨äººçŠ¶æ€"""
        # TODO: å®ç°å®é™…çš„FR3çŠ¶æ€è¯»å–
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        qpos = np.array([0.0, -0.5, 0.3, 0.0, -0.2, 0.1, 0.0, 0.05])
        return qpos
        
    def get_camera_images(self):
        """è·å–FR3ç›¸æœºå›¾åƒ"""
        # TODO: å®ç°å®é™…çš„FR3ç›¸æœºå›¾åƒè·å–
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        ee_img = np.random.rand(480, 640, 3).astype(np.uint8) * 255
        third_person_img = np.random.rand(480, 640, 3).astype(np.uint8) * 255
        
        return {
            'ee_cam': ee_img,
            'third_person_cam': third_person_img
        }
        
    def send_action_to_robot(self, action):
        """å‘é€åŠ¨ä½œåˆ°FR3æœºå™¨äºº"""
        # TODO: å®ç°å®é™…çš„FR3æœºå™¨äººæ§åˆ¶
        print(f"å‘é€åŠ¨ä½œåˆ°FR3æœºå™¨äºº: {action}")