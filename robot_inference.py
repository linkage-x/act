#!/usr/bin/env python3

import torch
import numpy as np
import h5py
import os
from abc import ABC, abstractmethod
from policy_inference import PolicyInference
from robot_config import RobotConfig, get_robot_config, detect_robot_from_task, detect_robot_from_checkpoint


class RobotInference(ABC):
    """æœºå™¨äººæ¨ç†æŠ½è±¡ç±»ï¼Œå®šä¹‰æœºå™¨äººç‰¹å®šçš„æ¥å£"""
    
    def __init__(self, policy_inference, task_config, robot_config):
        """
        åˆå§‹åŒ–æœºå™¨äººæ¨ç†å™¨
        
        Args:
            policy_inference: ç­–ç•¥æ¨ç†å™¨å®ä¾‹ (ACTInference, PPOInferenceç­‰)
            task_config: ä»»åŠ¡é…ç½®å­—å…¸
            robot_config: æœºå™¨äººé…ç½® (RobotConfigå®ä¾‹)
        """
        self.policy_inference = policy_inference
        self.task_config = task_config
        self.robot_config = robot_config
        
    @abstractmethod
    def preprocess_images(self, images_dict):
        """
        é¢„å¤„ç†å›¾åƒæ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼‰
        
        Args:
            images_dict: åŸå§‹å›¾åƒå­—å…¸
            
        Returns:
            processed_images: é¢„å¤„ç†åçš„å›¾åƒå­—å…¸
        """
        pass
        
    @abstractmethod
    def validate_input_data(self, state, images_dict):
        """
        éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼‰
        
        Args:
            state: æœºå™¨äººçŠ¶æ€
            images_dict: å›¾åƒå­—å…¸
            
        Raises:
            ValueError: å½“è¾“å…¥æ•°æ®æ ¼å¼ä¸æ­£ç¡®æ—¶
        """
        pass
        
    def predict(self, state, images_dict):
        """
        è¿›è¡ŒåŠ¨ä½œé¢„æµ‹
        
        Args:
            state: æœºå™¨äººçŠ¶æ€
            images_dict: ç›¸æœºå›¾åƒå­—å…¸
            
        Returns:
            predicted_actions: é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
        """
        # éªŒè¯è¾“å…¥æ•°æ®
        self.validate_input_data(state, images_dict)
        
        # é¢„å¤„ç†å›¾åƒ
        processed_images = self.preprocess_images(images_dict)
        
        # è°ƒç”¨ç­–ç•¥æ¨ç†
        return self.policy_inference.predict(state, processed_images)


class FR3Inference(RobotInference):
    """FR3æœºå™¨äººç‰¹å®šçš„æ¨ç†å™¨"""
    
    def __init__(self, policy_inference, task_name='fr3_peg_in_hole_extended'):
        """
        åˆå§‹åŒ–FR3æ¨ç†å™¨
        
        Args:
            policy_inference: ç­–ç•¥æ¨ç†å™¨å®ä¾‹
            task_name: ä»»åŠ¡åç§°
        """
        from constants import SIM_TASK_CONFIGS
        from robot_config import get_robot_config
        
        self.task_name = task_name
        
        if task_name not in SIM_TASK_CONFIGS:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡åç§°: {task_name}")
            
        task_config = SIM_TASK_CONFIGS[task_name]
        robot_config = get_robot_config('fr3')
        super().__init__(policy_inference, task_config, robot_config)
        
    def preprocess_images(self, images_dict):
        """é¢„å¤„ç†FR3å›¾åƒæ•°æ®"""
        processed_images = {}
        
        for cam_name, image in images_dict.items():
            if cam_name not in self.robot_config.camera_names:
                continue
                
            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡® (H, W, C) -> (C, H, W)
            if len(image.shape) == 3 and image.shape[-1] == 3:
                image = image.transpose(2, 0, 1)
                
            # å½’ä¸€åŒ–åˆ° [0, 1]
            if image.max() > 1.0:
                image = image.astype(np.float32) / 255.0
                
            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            image_tensor = torch.from_numpy(image).float().to(self.policy_inference.device)
            if len(image_tensor.shape) == 3:
                image_tensor = image_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                
            processed_images[cam_name] = image_tensor
            
        return processed_images
        
    def validate_input_data(self, qpos, images_dict):
        """éªŒè¯FR3è¾“å…¥æ•°æ®æ ¼å¼"""
        # éªŒè¯qposç»´åº¦
        qpos = np.array(qpos)
        if qpos.shape[0] != self.robot_config.dof:
            raise ValueError(f"QPosç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.robot_config.dof}, å¾—åˆ°{qpos.shape[0]}")
            
        # éªŒè¯ç›¸æœºæ•°é‡
        if len(images_dict) != len(self.robot_config.camera_names):
            raise ValueError(f"ç›¸æœºæ•°é‡ä¸åŒ¹é…: æœŸæœ›{self.robot_config.camera_names}, å¾—åˆ°{list(images_dict.keys())}")
            
        # éªŒè¯ç›¸æœºåç§°
        for cam_name in images_dict.keys():
            if cam_name not in self.robot_config.camera_names:
                raise ValueError(f"æœªçŸ¥ç›¸æœºåç§°: {cam_name}")
                
    def predict_from_episode(self, episode_path, timestep=0):
        """
        ä»episodeæ–‡ä»¶ä¸­è¯»å–æ•°æ®å¹¶è¿›è¡Œé¢„æµ‹
        
        Args:
            episode_path: HDF5 episodeæ–‡ä»¶è·¯å¾„
            timestep: è¦é¢„æµ‹çš„æ—¶é—´æ­¥
            
        Returns:
            predicted_actions: é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
            ground_truth: çœŸå®åŠ¨ä½œ (ç”¨äºå¯¹æ¯”)
            qpos: å…³èŠ‚ä½ç½®
        """
        with h5py.File(episode_path, 'r') as f:
            # è¯»å–æ•°æ®
            qpos = f['observations/qpos'][timestep]
            
            # æ„å»ºå›¾åƒå­—å…¸ - ä½¿ç”¨é…ç½®ä¸­çš„ç›¸æœºåç§°
            images_dict = {}
            for cam_name in self.robot_config.camera_names:
                if f'observations/images/{cam_name}' in f:
                    images_dict[cam_name] = f[f'observations/images/{cam_name}'][timestep]
                else:
                    raise KeyError(f"ç›¸æœºæ•°æ®ä¸å­˜åœ¨: observations/images/{cam_name}")
            
            # çœŸå®åŠ¨ä½œ (ç”¨äºå¯¹æ¯”)
            if timestep < f['action'].shape[0]:
                ground_truth = f['action'][timestep]
            else:
                ground_truth = None
                
        # è¿›è¡Œé¢„æµ‹
        predicted_actions = self.predict(qpos, images_dict)
        
        return predicted_actions, ground_truth, qpos
        
    def evaluate_episode(self, episode_path, num_steps=10):
        """
        è¯„ä¼°æ•´ä¸ªepisodeçš„é¢„æµ‹æ€§èƒ½
        
        Args:
            episode_path: HDF5 episodeæ–‡ä»¶è·¯å¾„
            num_steps: è¦è¯„ä¼°çš„æ­¥æ•°
            
        Returns:
            avg_mse: å¹³å‡MSEè¯¯å·®
        """
        print(f"\nğŸ“Š è¯„ä¼°Episode: {episode_path}")
        
        mse_errors = []
        
        with h5py.File(episode_path, 'r') as f:
            episode_len = min(f['action'].shape[0], num_steps)
            
        for t in range(episode_len):
            pred_actions, gt_action, qpos = self.predict_from_episode(episode_path, t)
            
            if gt_action is not None:
                # è®¡ç®—MSEè¯¯å·® (åªæ¯”è¾ƒç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œå› ä¸ºchunk_size=100)
                mse = np.mean((pred_actions[0] - gt_action) ** 2)
                mse_errors.append(mse)
                
                if t % 5 == 0:  # æ¯5æ­¥æ‰“å°ä¸€æ¬¡
                    print(f"  æ­¥éª¤ {t:3d}: MSE={mse:.6f}")
                    print(f"    é¢„æµ‹: {pred_actions[0]}")
                    print(f"    çœŸå®: {gt_action}")
                    
        avg_mse = np.mean(mse_errors)
        print(f"\nğŸ“ˆ å¹³å‡MSEè¯¯å·®: {avg_mse:.6f}")
        return avg_mse


class Monte01Inference(RobotInference):
    """Monte01åŒè‡‚æœºå™¨äººç‰¹å®šçš„æ¨ç†å™¨"""
    
    def __init__(self, policy_inference, task_name='monte01_peg_in_hole'):
        """
        åˆå§‹åŒ–Monte01æ¨ç†å™¨
        
        Args:
            policy_inference: ç­–ç•¥æ¨ç†å™¨å®ä¾‹
            task_name: ä»»åŠ¡åç§°
        """
        from constants import SIM_TASK_CONFIGS
        from robot_config import get_robot_config
        
        self.task_name = task_name
        
        if task_name not in SIM_TASK_CONFIGS:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡åç§°: {task_name}")
            
        task_config = SIM_TASK_CONFIGS[task_name]
        robot_config = get_robot_config('monte01')
        super().__init__(policy_inference, task_config, robot_config)
        
    def preprocess_images(self, images_dict):
        """é¢„å¤„ç†Monte01å›¾åƒæ•°æ® (æ”¯æŒä¸‰ä¸ªç›¸æœº)"""
        processed_images = {}
        
        for cam_name, image in images_dict.items():
            if cam_name not in self.robot_config.camera_names:
                continue
                
            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡® (H, W, C) -> (C, H, W)
            if len(image.shape) == 3 and image.shape[-1] == 3:
                image = image.transpose(2, 0, 1)
                
            # å½’ä¸€åŒ–åˆ° [0, 1]
            if image.max() > 1.0:
                image = image.astype(np.float32) / 255.0
                
            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            image_tensor = torch.from_numpy(image).float().to(self.policy_inference.device)
            if len(image_tensor.shape) == 3:
                image_tensor = image_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                
            processed_images[cam_name] = image_tensor
            
        return processed_images
        
    def validate_input_data(self, qpos, images_dict):
        """éªŒè¯Monte01è¾“å…¥æ•°æ®æ ¼å¼"""
        # éªŒè¯qposç»´åº¦ (16 DOF)
        qpos = np.array(qpos)
        if qpos.shape[0] != self.robot_config.dof:
            raise ValueError(f"QPosç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.robot_config.dof}, å¾—åˆ°{qpos.shape[0]}")
            
        # éªŒè¯ç›¸æœºæ•°é‡ (æ”¯æŒç¼ºå¤±ç›¸æœº)
        required_cameras = set(self.robot_config.camera_names)
        available_cameras = set(images_dict.keys())
        
        if not available_cameras.issubset(required_cameras):
            unknown_cams = available_cameras - required_cameras
            raise ValueError(f"æœªçŸ¥ç›¸æœºåç§°: {unknown_cams}")
                
    def predict_from_episode(self, episode_path, timestep=0):
        """
        ä»episodeæ–‡ä»¶ä¸­è¯»å–æ•°æ®å¹¶è¿›è¡Œé¢„æµ‹ (åŒè‡‚ç‰ˆæœ¬)
        
        Args:
            episode_path: HDF5 episodeæ–‡ä»¶è·¯å¾„
            timestep: è¦é¢„æµ‹çš„æ—¶é—´æ­¥
            
        Returns:
            predicted_actions: é¢„æµ‹çš„åŠ¨ä½œåºåˆ— (16 DOF)
            ground_truth: çœŸå®åŠ¨ä½œ (ç”¨äºå¯¹æ¯”)
            qpos: å…³èŠ‚ä½ç½® (16 DOF)
        """
        with h5py.File(episode_path, 'r') as f:
            # è¯»å–æ•°æ®
            qpos = f['observations/qpos'][timestep]
            
            # æ„å»ºå›¾åƒå­—å…¸ - ä½¿ç”¨é…ç½®ä¸­çš„ç›¸æœºåç§°ï¼Œæ”¯æŒç¼ºå¤±ç›¸æœº
            images_dict = {}
            for cam_name in self.robot_config.camera_names:
                if f'observations/images/{cam_name}' in f:
                    images_dict[cam_name] = f[f'observations/images/{cam_name}'][timestep]
                else:
                    print(f"âš ï¸ ç›¸æœºæ•°æ®ç¼ºå¤±: {cam_name}, å°†ä½¿ç”¨é›¶å›¾åƒ")
                    # åˆ›å»ºé›¶å›¾åƒ
                    images_dict[cam_name] = np.zeros((480, 640, 3), dtype=np.uint8)
            
            # çœŸå®åŠ¨ä½œ (ç”¨äºå¯¹æ¯”)
            if timestep < f['action'].shape[0]:
                ground_truth = f['action'][timestep]
            else:
                ground_truth = None
                
        # è¿›è¡Œé¢„æµ‹
        predicted_actions = self.predict(qpos, images_dict)
        
        return predicted_actions, ground_truth, qpos
        
    def split_prediction(self, actions):
        """
        å°†åŒè‡‚åŠ¨ä½œåˆ†ç¦»ä¸ºå·¦è‡‚å’Œå³è‡‚
        
        Args:
            actions: ç»„åˆåŠ¨ä½œ (16 DOF)
            
        Returns:
            left_actions: å·¦è‡‚åŠ¨ä½œ (8 DOF)
            right_actions: å³è‡‚åŠ¨ä½œ (8 DOF)
        """
        return self.robot_config.split_state(actions)
        
    def combine_actions(self, left_actions, right_actions):
        """
        ç»„åˆå·¦è‡‚å’Œå³è‡‚åŠ¨ä½œ
        
        Args:
            left_actions: å·¦è‡‚åŠ¨ä½œ (8 DOF)
            right_actions: å³è‡‚åŠ¨ä½œ (8 DOF)
            
        Returns:
            combined_actions: ç»„åˆåŠ¨ä½œ (16 DOF)
        """
        return self.robot_config.combine_state(left_actions, right_actions)
        
    def evaluate_episode(self, episode_path, num_steps=10):
        """
        è¯„ä¼°æ•´ä¸ªepisodeçš„é¢„æµ‹æ€§èƒ½ (åŒè‡‚ç‰ˆæœ¬)
        
        Args:
            episode_path: HDF5 episodeæ–‡ä»¶è·¯å¾„
            num_steps: è¦è¯„ä¼°çš„æ­¥æ•°
            
        Returns:
            avg_mse: å¹³å‡MSEè¯¯å·®
        """
        print(f"\nğŸ“Š è¯„ä¼°Monte01 Episode: {episode_path}")
        
        mse_errors = []
        left_mse_errors = []
        right_mse_errors = []
        
        with h5py.File(episode_path, 'r') as f:
            episode_len = min(f['action'].shape[0], num_steps)
            
        for t in range(episode_len):
            pred_actions, gt_action, qpos = self.predict_from_episode(episode_path, t)
            
            if gt_action is not None:
                # è®¡ç®—æ•´ä½“MSEè¯¯å·®
                mse = np.mean((pred_actions[0] - gt_action) ** 2)
                mse_errors.append(mse)
                
                # åˆ†åˆ«è®¡ç®—å·¦å³è‡‚MSE
                pred_left, pred_right = self.split_prediction(pred_actions[0])
                gt_left, gt_right = self.split_prediction(gt_action)
                
                left_mse = np.mean((pred_left - gt_left) ** 2)
                right_mse = np.mean((pred_right - gt_right) ** 2)
                
                left_mse_errors.append(left_mse)
                right_mse_errors.append(right_mse)
                
                if t % 5 == 0:  # æ¯5æ­¥æ‰“å°ä¸€æ¬¡
                    print(f"  æ­¥éª¤ {t:3d}: æ€»MSE={mse:.6f}, å·¦è‡‚={left_mse:.6f}, å³è‡‚={right_mse:.6f}")
                    
        avg_mse = np.mean(mse_errors)
        avg_left_mse = np.mean(left_mse_errors) if left_mse_errors else 0.0
        avg_right_mse = np.mean(right_mse_errors) if right_mse_errors else 0.0
        
        print(f"\nğŸ“ˆ å¹³å‡MSEè¯¯å·®:")
        print(f"   - æ€»ä½“: {avg_mse:.6f}")
        print(f"   - å·¦è‡‚: {avg_left_mse:.6f}")
        print(f"   - å³è‡‚: {avg_right_mse:.6f}")
        
        return avg_mse


class RealtimeController:
    """é€šç”¨å®æ—¶æ§åˆ¶å™¨"""
    
    def __init__(self, robot_inference):
        """
        åˆå§‹åŒ–å®æ—¶æ§åˆ¶å™¨
        
        Args:
            robot_inference: æœºå™¨äººæ¨ç†å™¨å®ä¾‹
        """
        self.robot_inference = robot_inference
        self.action_buffer = None  # åŠ¨ä½œç¼“å†²åŒº
        self.buffer_index = 0      # å½“å‰åŠ¨ä½œç´¢å¼•
        
        print("ğŸ¤– å®æ—¶æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def get_robot_state(self):
        """
        è·å–æœºå™¨äººå½“å‰çŠ¶æ€ (éœ€è¦æ ¹æ®å®é™…æœºå™¨äººæ¥å£å®ç°)
        
        Returns:
            state: æœºå™¨äººçŠ¶æ€
        """
        raise NotImplementedError("éœ€è¦å­ç±»å®ç°get_robot_stateæ–¹æ³•")
        
    def get_camera_images(self):
        """
        è·å–ç›¸æœºå›¾åƒ (éœ€è¦æ ¹æ®å®é™…æœºå™¨äººæ¥å£å®ç°)
        
        Returns:
            images_dict: å›¾åƒå­—å…¸
        """
        raise NotImplementedError("éœ€è¦å­ç±»å®ç°get_camera_imagesæ–¹æ³•")
        
    def send_action_to_robot(self, action):
        """
        å‘é€åŠ¨ä½œåˆ°æœºå™¨äºº (éœ€è¦æ ¹æ®å®é™…æœºå™¨äººæ¥å£å®ç°)
        
        Args:
            action: åŠ¨ä½œå‘é‡
        """
        raise NotImplementedError("éœ€è¦å­ç±»å®ç°send_action_to_robotæ–¹æ³•")
        
    def step(self):
        """æ‰§è¡Œä¸€æ­¥æ§åˆ¶å¾ªç¯"""
        # å¦‚æœåŠ¨ä½œç¼“å†²åŒºä¸ºç©ºæˆ–å·²ç”¨å®Œï¼Œé‡æ–°é¢„æµ‹
        if self.action_buffer is None or self.buffer_index >= self.action_buffer.shape[0]:
            state = self.get_robot_state()
            images = self.get_camera_images()
            
            # é¢„æµ‹åŠ¨ä½œåºåˆ—
            self.action_buffer = self.robot_inference.predict(state, images)
            self.buffer_index = 0
            
            print(f"ğŸ”„ é‡æ–°é¢„æµ‹åŠ¨ä½œåºåˆ—ï¼Œé•¿åº¦: {self.action_buffer.shape[0]}")
            
        # æ‰§è¡Œå½“å‰åŠ¨ä½œ
        current_action = self.action_buffer[self.buffer_index]
        self.send_action_to_robot(current_action)
        
        self.buffer_index += 1
        
        return current_action
        
    def run_realtime_control(self, max_steps=1000, freq_hz=10):
        """
        è¿è¡Œå®æ—¶æ§åˆ¶å¾ªç¯
        
        Args:
            max_steps: æœ€å¤§æ­¥æ•°
            freq_hz: æ§åˆ¶é¢‘ç‡ (Hz)
        """
        import time
        
        dt = 1.0 / freq_hz
        
        print(f"ğŸš€ å¼€å§‹å®æ—¶æ§åˆ¶ï¼Œé¢‘ç‡: {freq_hz}Hz")
        
        for step in range(max_steps):
            start_time = time.time()
            
            # æ‰§è¡Œä¸€æ­¥æ§åˆ¶
            action = self.step()
            
            # æ§åˆ¶é¢‘ç‡
            elapsed = time.time() - start_time
            if elapsed < dt:
                time.sleep(dt - elapsed)
                
            if step % 50 == 0:
                print(f"æ­¥éª¤ {step}: åŠ¨ä½œ = {action}")
                
        print("âœ… å®æ—¶æ§åˆ¶å®Œæˆ")


class FR3RealtimeController(RealtimeController):
    """FR3ç‰¹å®šçš„å®æ—¶æ§åˆ¶å™¨"""
    
    def get_robot_state(self):
        """è·å–FR3æœºå™¨äººçŠ¶æ€"""
        # TODO: å®ç°å®é™…çš„FR3çŠ¶æ€è¯»å–
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        qpos = np.array([0.0, -0.5, 0.3, 0.0, -0.2, 0.1, 0.0, 0.05])
        return qpos
        
    def get_camera_images(self):
        """è·å–FR3ç›¸æœºå›¾åƒ"""
        # TODO: å®ç°å®é™…çš„FR3ç›¸æœºå›¾åƒè·å–
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        ee_img = np.random.rand(480, 640, 3).astype(np.uint8) * 255
        third_person_img = np.random.rand(480, 640, 3).astype(np.uint8) * 255
        
        return {
            'ee_cam': ee_img,
            'third_person_cam': third_person_img
        }
        
    def send_action_to_robot(self, action):
        """å‘é€åŠ¨ä½œåˆ°FR3æœºå™¨äºº"""
        # TODO: å®ç°å®é™…çš„FR3æœºå™¨äººæ§åˆ¶
        print(f"å‘é€åŠ¨ä½œåˆ°FR3æœºå™¨äºº: {action}")


class Monte01RealtimeController(RealtimeController):
    """Monte01åŒè‡‚æœºå™¨äººç‰¹å®šçš„å®æ—¶æ§åˆ¶å™¨"""
    
    def get_robot_state(self):
        """è·å–Monte01æœºå™¨äººçŠ¶æ€ (16 DOF)"""
        # TODO: å®ç°å®é™…çš„Monte01çŠ¶æ€è¯»å–
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        left_arm = np.array([0.0, -0.5, 0.3, 0.0, -0.2, 0.1, 0.0, 0.037])  # 8 DOF
        right_arm = np.array([0.0, -0.5, 0.3, 0.0, -0.2, 0.1, 0.0, 0.037])  # 8 DOF
        qpos = np.concatenate([left_arm, right_arm])  # 16 DOF
        return qpos
        
    def get_camera_images(self):
        """è·å–Monte01ç›¸æœºå›¾åƒ (ä¸‰ä¸ªç›¸æœº)"""
        # TODO: å®ç°å®é™…çš„Monte01ç›¸æœºå›¾åƒè·å–
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        ee_img = np.random.rand(480, 640, 3).astype(np.uint8) * 255
        right_ee_img = np.random.rand(480, 640, 3).astype(np.uint8) * 255
        third_person_img = np.random.rand(480, 640, 3).astype(np.uint8) * 255
        
        return {
            'ee_cam': ee_img,
            'right_ee_cam': right_ee_img,
            'third_person_cam': third_person_img
        }
        
    def send_action_to_robot(self, action):
        """å‘é€åŠ¨ä½œåˆ°Monte01æœºå™¨äºº (16 DOF)"""
        # TODO: å®ç°å®é™…çš„Monte01æœºå™¨äººæ§åˆ¶
        left_action = action[:8]
        right_action = action[8:16]
        print(f"å‘é€åŠ¨ä½œåˆ°Monte01æœºå™¨äºº:")
        print(f"  å·¦è‡‚: {left_action}")
        print(f"  å³è‡‚: {right_action}")


def create_robot_inference(ckpt_dir, task_name=None, robot_type=None):
    """
    å·¥å‚å‡½æ•°ï¼šè‡ªåŠ¨åˆ›å»ºåˆé€‚çš„æœºå™¨äººæ¨ç†å™¨
    
    Args:
        ckpt_dir: æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„
        task_name: ä»»åŠ¡åç§° (å¯é€‰ï¼Œç”¨äºè‡ªåŠ¨æ£€æµ‹)
        robot_type: æœºå™¨äººç±»å‹ (å¯é€‰ï¼Œ'fr3' æˆ– 'monte01')
        
    Returns:
        robot_inference: æœºå™¨äººæ¨ç†å™¨å®ä¾‹
    """
    from policy_inference import ACTInference
    from robot_config import detect_robot_from_checkpoint, detect_robot_from_task
    
    # è‡ªåŠ¨æ£€æµ‹æœºå™¨äººç±»å‹
    if robot_type is None:
        if task_name:
            robot_type = detect_robot_from_task(task_name)
        else:
            robot_type = detect_robot_from_checkpoint(ckpt_dir)
            
    print(f"ğŸ¤– æ£€æµ‹åˆ°æœºå™¨äººç±»å‹: {robot_type}")
    
    # è·å–æœºå™¨äººé…ç½®
    robot_config = get_robot_config(robot_type)
    
    # åˆ›å»ºç­–ç•¥æ¨ç†å™¨
    policy_inference = ACTInference(
        ckpt_dir=ckpt_dir,
        state_dim=robot_config.dof,
        camera_names=robot_config.camera_names
    )
    
    # åˆ›å»ºæœºå™¨äººæ¨ç†å™¨
    if robot_type == 'fr3':
        if not task_name:
            task_name = 'fr3_peg_in_hole'
        robot_inference = FR3Inference(policy_inference, task_name)
    elif robot_type == 'monte01':
        if not task_name:
            task_name = 'monte01_peg_in_hole'
        robot_inference = Monte01Inference(policy_inference, task_name)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æœºå™¨äººç±»å‹: {robot_type}")
        
    print(f"âœ… {robot_type.upper()} æ¨ç†å™¨åˆ›å»ºå®Œæˆ")
    print(f"   - ä»»åŠ¡: {task_name}")
    print(f"   - DOF: {robot_config.dof}")
    print(f"   - ç›¸æœº: {robot_config.camera_names}")
    
    return robot_inference


def create_realtime_controller(robot_inference):
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºåˆé€‚çš„å®æ—¶æ§åˆ¶å™¨
    
    Args:
        robot_inference: æœºå™¨äººæ¨ç†å™¨å®ä¾‹
        
    Returns:
        realtime_controller: å®æ—¶æ§åˆ¶å™¨å®ä¾‹
    """
    if isinstance(robot_inference, FR3Inference):
        return FR3RealtimeController(robot_inference)
    elif isinstance(robot_inference, Monte01Inference):
        return Monte01RealtimeController(robot_inference)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨ç†å™¨ç±»å‹: {type(robot_inference)}")


def main():
    """æµ‹è¯•è„šæœ¬"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æœºå™¨äººæ¨ç†æµ‹è¯•')
    parser.add_argument('--ckpt_dir', type=str, required=True,
                       help='æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„')
    parser.add_argument('--task_name', type=str, default=None,
                       help='ä»»åŠ¡åç§°')
    parser.add_argument('--robot_type', type=str, default=None,
                       choices=['fr3', 'monte01'], help='æœºå™¨äººç±»å‹')
    parser.add_argument('--episode', type=str, default=None,
                       help='æµ‹è¯•episodeæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_steps', type=int, default=20,
                       help='è¯„ä¼°æ­¥æ•°')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæœºå™¨äººæ¨ç†å™¨
        robot_inference = create_robot_inference(
            args.ckpt_dir, 
            args.task_name, 
            args.robot_type
        )
        
        if args.episode:
            # æµ‹è¯•episodeæ–‡ä»¶
            print(f"\nğŸ¯ æµ‹è¯•episodeæ–‡ä»¶: {args.episode}")
            avg_mse = robot_inference.evaluate_episode(args.episode, args.num_steps)
            print(f"\nâœ… æµ‹è¯•å®Œæˆ! å¹³å‡MSE: {avg_mse:.6f}")
        else:
            # åˆ›å»ºå®æ—¶æ§åˆ¶å™¨æµ‹è¯•
            print(f"\nğŸ® åˆ›å»ºå®æ—¶æ§åˆ¶å™¨æµ‹è¯•")
            controller = create_realtime_controller(robot_inference)
            print(f"âœ… å®æ—¶æ§åˆ¶å™¨åˆ›å»ºæˆåŠŸ: {type(controller).__name__}")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()