#!/usr/bin/env python3

import torch
import numpy as np
import pickle
import os
from abc import ABC, abstractmethod


class PolicyInference(ABC):
    """é€šç”¨æ¨ç†ç®—æ³•åŸºç±»ï¼ŒæŠ½è±¡å‡ºæ¨ç†ç®—æ³•æ¥å£"""
    
    def __init__(self, ckpt_dir, policy_config):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            ckpt_dir: æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„
            policy_config: ç­–ç•¥é…ç½®å­—å…¸
        """
        self.ckpt_dir = ckpt_dir
        self.policy_config = policy_config
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        self.load_dataset_stats()
        
        # åˆå§‹åŒ–ç­–ç•¥
        self.init_policy()
        
        # åŠ è½½æ¨¡å‹
        self.load_policy()
        
    def load_dataset_stats(self):
        """åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ç”¨äºå½’ä¸€åŒ–"""
        stats_path = os.path.join(self.ckpt_dir, 'dataset_stats.pkl')
        
        if not os.path.exists(stats_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®é›†ç»Ÿè®¡æ–‡ä»¶: {stats_path}")
            
        with open(stats_path, 'rb') as f:
            self.dataset_stats = pickle.load(f)
            
        print(f"âœ… åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
        print(f"   - ç»Ÿè®¡ä¿¡æ¯é”®: {list(self.dataset_stats.keys())}")
        
    @abstractmethod
    def init_policy(self):
        """åˆå§‹åŒ–ç­–ç•¥æ¨¡å‹ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼‰"""
        pass
        
    @abstractmethod
    def load_policy(self):
        """åŠ è½½è®­ç»ƒå¥½çš„ç­–ç•¥æ¨¡å‹ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼‰"""
        pass
        
    @abstractmethod
    def forward_policy(self, state, images):
        """
        ç­–ç•¥å‰å‘æ¨ç†ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼‰
        
        Args:
            state: çŠ¶æ€å¼ é‡
            images: å›¾åƒå¼ é‡
            
        Returns:
            actions: é¢„æµ‹çš„åŠ¨ä½œ
        """
        pass
        
    def normalize_data(self, data, stats, key):
        """æ•°æ®å½’ä¸€åŒ–
        æ”¯æŒ stats ä¸­ä¸º torch.Tensor æˆ– numpy.ndarrayï¼Œä¸¤è€…ä¸ data ç±»å‹å¯¹é½
        """
        mean = stats[f'{key}_mean']
        std = stats[f'{key}_std']
        # Align types with input data (numpy path)
        if isinstance(data, np.ndarray):
            if hasattr(mean, 'detach'):
                mean = mean.detach().cpu().numpy()
            if hasattr(std, 'detach'):
                std = std.detach().cpu().numpy()
            return (data - mean) / std
        # Torch path
        if not isinstance(data, torch.Tensor):
            data = torch.as_tensor(data)
        if not isinstance(mean, torch.Tensor):
            mean = torch.as_tensor(mean)
        if not isinstance(std, torch.Tensor):
            std = torch.as_tensor(std)
        return (data - mean) / std
        
    def denormalize_data(self, data, stats, key):
        """æ•°æ®åå½’ä¸€åŒ–
        æ”¯æŒ stats ä¸­ä¸º torch.Tensor æˆ– numpy.ndarrayï¼Œä¸¤è€…ä¸ data ç±»å‹å¯¹é½
        """
        mean = stats[f'{key}_mean']
        std = stats[f'{key}_std']
        # Align types with input data
        if isinstance(data, np.ndarray):
            if hasattr(mean, 'detach'):
                mean = mean.detach().cpu().numpy()
            if hasattr(std, 'detach'):
                std = std.detach().cpu().numpy()
            return data * std + mean
        # Torch path
        if not isinstance(data, torch.Tensor):
            data = torch.as_tensor(data)
        if not isinstance(mean, torch.Tensor):
            mean = torch.as_tensor(mean)
        if not isinstance(std, torch.Tensor):
            std = torch.as_tensor(std)
        return data * std + mean
        
    def predict(self, state, images):
        """
        è¿›è¡ŒåŠ¨ä½œé¢„æµ‹

        Args:
            state: çŠ¶æ€å‘é‡
            images: å›¾åƒæ•°æ®

        Returns:
            predicted_actions: é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
        """
        with torch.no_grad():
            # é¢„å¤„ç†çŠ¶æ€
            state = np.array(state)

            # ğŸ”¥ å…³é”®ä¿®å¤: æ ¹æ®æ§åˆ¶æ¨¡å¼é€‰æ‹©æ­£ç¡®çš„å½’ä¸€åŒ–é”®
            # æ£€æŸ¥æ˜¯å¦ä¸ºEE poseæ§åˆ¶æ¨¡å¼
            if self.dataset_stats.get('has_ee_pose', False):
                # EEæ§åˆ¶æ¨¡å¼: ä½¿ç”¨ee_poseç»Ÿè®¡ä¿¡æ¯
                state_key = 'ee_pose'
                action_key = 'ee_action'
                print(f"ğŸ¯ Using EE pose control mode for normalization")
            else:
                # å…³èŠ‚æ§åˆ¶æ¨¡å¼: ä½¿ç”¨qposç»Ÿè®¡ä¿¡æ¯
                state_key = 'qpos'
                action_key = 'action'
                print(f"ğŸ¯ Using joint control mode for normalization")

            state_normalized = self.normalize_data(state, self.dataset_stats, state_key)
            state_tensor = torch.from_numpy(state_normalized).float().to(self.device)
            state_tensor = state_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦

            # ç­–ç•¥æ¨ç†
            actions = self.forward_policy(state_tensor, images)

            # åå½’ä¸€åŒ–åŠ¨ä½œ - ä½¿ç”¨å¯¹åº”çš„action key
            actions_np = actions.cpu().numpy().squeeze(0)  # ç§»é™¤batchç»´åº¦
            actions_denorm = self.denormalize_data(actions_np, self.dataset_stats, action_key)

            return actions_denorm


class ACTInference(PolicyInference):
    """ACTç®—æ³•ç‰¹å®šçš„æ¨ç†å™¨"""
    
    def __init__(self, ckpt_dir, state_dim, camera_names, **kwargs):
        """
        åˆå§‹åŒ–ACTæ¨ç†å™¨
        
        Args:
            ckpt_dir: æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„
            state_dim: çŠ¶æ€ç»´åº¦
            camera_names: ç›¸æœºåç§°åˆ—è¡¨
            **kwargs: å…¶ä»–ACTå‚æ•°
        """
        # ACTç­–ç•¥é…ç½® - ç¡®ä¿æ•°å€¼å‚æ•°ä¸ºæ­£ç¡®ç±»å‹
        policy_config = {
            'lr': float(kwargs.get('lr', 1e-5)),  # ç¡®ä¿ lr ä¸ºæµ®ç‚¹æ•°
            'num_queries': int(kwargs.get('num_queries', 100)),  # chunk_size
            'kl_weight': int(kwargs.get('kl_weight', 10)),
            'hidden_dim': int(kwargs.get('hidden_dim', 512)),
            'dim_feedforward': int(kwargs.get('dim_feedforward', 3200)),
            'lr_backbone': float(kwargs.get('lr_backbone', 1e-5)),  # ç¡®ä¿ lr_backbone ä¸ºæµ®ç‚¹æ•°
            'backbone': str(kwargs.get('backbone', 'resnet18')),
            'enc_layers': int(kwargs.get('enc_layers', 4)),
            'dec_layers': int(kwargs.get('dec_layers', 7)),
            'nheads': int(kwargs.get('nheads', 8)),
            'camera_names': camera_names,
            'vq': bool(kwargs.get('vq', False)),
            'vq_class': kwargs.get('vq_class', None),
            'vq_dim': kwargs.get('vq_dim', None) if kwargs.get('vq_dim') is None else int(kwargs.get('vq_dim')),
            'action_dim': int(state_dim),
            'no_encoder': bool(kwargs.get('no_encoder', False)),
        }
        
        super().__init__(ckpt_dir, policy_config)
        
    def init_policy(self):
        """åˆå§‹åŒ–ACTç­–ç•¥"""
        from .policy import ACTPolicy
        
        self.policy = ACTPolicy(self.policy_config)
        self.policy.to(self.device)
        self.policy.eval()
        
        print(f"âœ… ACTç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
        print(f"   - Stateç»´åº¦: {self.policy_config['action_dim']}")
        print(f"   - Actionå—å¤§å°: {self.policy_config['num_queries']}")
        print(f"   - ç›¸æœº: {self.policy_config['camera_names']}")
        
    def load_policy(self):
        """åŠ è½½ACTç­–ç•¥æ¨¡å‹"""
        policy_path = os.path.join(self.ckpt_dir, 'policy_best.ckpt')
        
        if not os.path.exists(policy_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç­–ç•¥æ–‡ä»¶: {policy_path}")
            
        checkpoint = torch.load(policy_path, map_location=self.device)
        self.policy.load_state_dict(checkpoint)
        
        print(f"âœ… åŠ è½½ACTç­–ç•¥æ¨¡å‹: {policy_path}")
        
    def forward_policy(self, state, images):
        """ACTç­–ç•¥å‰å‘æ¨ç†"""
        return self.policy(state, images)
